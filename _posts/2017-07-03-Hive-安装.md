---
layout: post
title: "Hive Install"
description: "Hive Install"
category: articles
tags: [Spark]
comments: true
---
原文链接:
<http://www.jianshu.com/p/a7f75b868568>
<http://blog.csdn.net/yirenboy/article/details/47446211>
<https://dev.mysql.com/doc/mysql-yum-repo-quick-guide/en/# repo-qg-yum-fresh-install>
<https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started>

Hive Install
==================
Hive安装在nn1,mysql安装在dn1

#### 问题1
**启动metastore时报错**
```
[root@nn1 conf]#  hive --service metastore &
[1] 7131
[root@nn1 conf]#  which: no hbase in (/opt/scala/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/usr/java/default/bin:/usr/java/default/bin:/app/hadoop/sbin:/app/hadoop/bin:/app/zookeeper-3.4.10/bin:/app/spark/bin:/app/spark/sbin:/app/hive/bin)
Starting Hive Metastore Server
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html# multiple_bindings for an explanation.
。。。。
javax.jdo.JDODataStoreException: Required table missing : "`DBS`" in Catalog "" Schema "". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable "datanucleus.schema.autoCreateTables"
```
**方案**
1. 修改hive-site.xml，在连接串中加入指定SSL为false即可：
```
<property>
   <name>javax.jdo.option.ConnectionURL</name>
   <value>jdbc:mysql://dn1:3306/hive?=createDatabaseIfNotExist=true;&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
   <description>
     JDBC connect string for a JDBC metastore.
     To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
     For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
   </description>
 </property>
```
2. 初始化schema
[root@nn1 conf]#  $HIVE_HOME/bin/schematool -dbType mysql  -initSchema
#### 问题2
**启动hiveserver报错**
```
[root@nn1 conf]#  hive --service hiveserver &
[2] 7800
[root@nn1 conf]#  which: no hbase in (/opt/scala/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/usr/java/default/bin:/usr/java/default/bin:/app/hadoop/sbin:/app/hadoop/bin:/app/zookeeper-3.4.10/bin:/app/spark/bin:/app/spark/sbin:/app/hive/bin)
Service hiveserver not found
Available Services: beeline cleardanglingscratchdir cli hbaseimport hbaseschematool help hiveburninclient hiveserver2 hplsql hwi jar lineage llapdump llap llapstatus metastore metatool orcfiledump rcfilecat schemaTool version

[2]+  Exit 7                  hive --service hiveserver
```
**方案**
原来 hiveserver 已经被 hiveserver2 取代了，详细可参考：
[root@nn1 conf]#  hive --service hiveserver2 &
#### 问题3
**执行hive报错**
```
[root@nn1 conf]#  hive
which: no hbase in (/opt/scala/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/usr/java/default/bin:/usr/java/default/bin:/app/hadoop/sbin:/app/hadoop/bin:/app/zookeeper-3.4.10/bin:/app/spark/bin:/app/spark/sbin:/app/hive/bin)
.....
Exception in thread "main" java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException:
Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
       at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:591)
       at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:531)
       at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)
       at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
       at java.lang.reflect.Method.invoke(Method.java:498)
       at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
...
Caused by: java.net.ConnectException: Connection refused (Connection refused)
```
**方案**
我理解错了，metastore应该是在nn1上面的，把配置修改为下：
```
<property>
    <name>hive.metastore.uris</name>
<value>thrift://nn1:9083</value>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
  </property>
```
#### 问题4
**也是执行hive出错，如下：**
```
[root@nn1 conf]#  hive
which: no hbase in (/opt/scala/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/usr/java/default/bin:/usr/java/default/bin:/app/hadoop/sbin:/app/hadoop/bin:/app/zookeeper-3.4.10/bin:/app/spark/bin:/app/spark/sbin:/app/hive/bin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html# multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/app/hive/lib/hive-common-2.1.1.jar!/hive-log4j2.properties Async: true
Exception in thread "main" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at org.apache.hadoop.fs.Path.initialize(Path.java:205)
        at org.apache.hadoop.fs.Path.<init>(Path.java:171)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:644)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:563)
        at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:531)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at java.net.URI.checkPath(URI.java:1823)
        at java.net.URI.<init>(URI.java:745)
        at org.apache.hadoop.fs.Path.initialize(Path.java:202)
        ... 12 more
```
**方案**
Change in hfs-site.xml this properties
```
<name>hive.exec.scratchdir</name>
<value>/tmp/hive-${user.name}</value>

 <name>hive.exec.local.scratchdir</name>
 <value>/tmp/${user.name}</value>

<name>hive.downloaded.resources.dir</name>
<value>/tmp/${user.name}_resources</value>

<name>hive.scratch.dir.permission</name>
    <value>733</value>
```
#### 问题5
**也是执行hive**
```
[root@nn1 conf]#  hive
...
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
```
**方案**
因为历史原因，目前hive的默认Engine还是MR，但是呢，是准备remove掉的。因为我想用spark，所以参照这个<https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started>,修改了hive-site.xml如下：
```
<property>
   <name>hive.execution.engine</name>
   <value>spark</value>
   <description>
     Expects one of [mr, tez, spark].
     Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR
     remains the default engine for historical reasons, it is itself a historical engine
     and is deprecated in Hive 2 line. It may be removed without further warning.
   </description>
 </property>
```
#### 问题6
**Insert时抛异常**
```
hive> insert into student values(1,"xiaoli","F",23);
Exception in thread "main" java.lang.NoClassDefFoundError: scala/collection/Iterable
        at org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.<init>(GenSparkProcContext.java:163)
        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:195)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:267)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10947)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:246)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:477)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1242)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1384)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1171)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1161)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.ClassNotFoundException: scala.collection.Iterable
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 23 more
```
**方案**
Refer to :<https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started>
```
[root@nn1 hive]#  ln -s /app/spark/jars/scala-library-2.11.8.jar /app/hive/lib/
[root@nn1 hive]#  ln -s /app/spark/jars/spark-core_2.11-2.1.1.jar /app/hive/lib/
[root@nn1 hive]#  ln -s /app/spark/jars/spark-network-common_2.11-2.1.1.jar /app/hive/lib/
[root@nn1 hive]#  hadoop fs -mkdir /spark-jars
[root@nn1 hive]#   hadoop fs -copyFromLocal /app/spark/jars/* /spark-jars
```
添加如下选项去hive-site.xml
```
<property>
  <name>spark.yarn.jars</name>
  <value>hdfs://nn1:9000/spark-jars/*</value>
</property>
```
#### 问题7
```
hive> insert into student values(1,'Xueqian','F',23);
Query ID = root_20170704103037_a1d3cad3-d8da-4d46-85bd-92ea1f23c153
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
hive>
```
**原因**</br>
Hive On Spark运行时，Hive需要启动一个程序连接Spark集群，因为Hive版本和Spark版本不匹配的原因，或者是配置不对的原因导致Hive连不上Spark集群，无法提交Spark  Job都会报这个错误。

**方案**</br>
使用debug模式看具体原因：
```
[root@nn1 conf]#  hive  --hiveconf  hive.root.logger=DEBUG,console  -e  "insert into sparktest.student values(1,'Xueqian','F',23);"
```
发现了如下异常：
```
java.lang.NoClassDefFoundError: org/apache/spark/JavaSparkListener
```
网上一搜，才发现是hive和spark不兼容的原因。。。
我的spark是version 2.1.1， hive是2.1.1。 然后我就没得办法了撒。只有参考<http://dblab.xmu.edu.cn/blog/1086-2/>重新下载一个spark，专门为了hive的学习。

#### 专门为了hive的spark1的启动
因为有两个spark，所以这一个spark启动前，要这样：
```
[root@nn1 conf]#  export SPARK_HOME=/app/spark-2.0.2-bin-hadoop2-without-hive/
[root@nn1 conf]#  ../sbin/start-all.sh
```

#### 验证安装
```
[root@nn1 ~]#  hive --service metastore &
[root@nn1 ~]#   hive --service hiveserver2 &
[root@nn1 ~]#  hive
