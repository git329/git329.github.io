---
layout: post
title: Spark错误集锦
description: "Spark编程"
category: articles
tags: [Spark编程]
comments: true
---
Spark-我遇到的那些傻错误
======================
#### 错误1
```
scala> val text=sc.textFile("hdfs://nn1:50070/sogou/")
text: org.apache.spark.rdd.RDD[String] = hdfs://nn1:50070/sogou/ MapPartitionsRDD[4] at textFile at <console>:24

scala> text.toD
toDF   toDS   toDebugString

scala> text.toDebugString
java.io.IOException: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: "nn1/10.255.20.190"; destination host is: "nn1":50070;
  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:776)
  at org.apache.hadoop.ipc.Client.call(Client.java:1479)
```
**原因：**</br>
原因比较瓜，是我用错端口了，如上的nn1:50070是http的端口，应该用rpc的端口
```
<property>
   <name>dfs.namenode.rpc-address.first-cluster.nn1</name>
   <value>nn1:9000</value>
 </property>
 <property>
   <name>dfs.namenode.http-address.first-cluster.nn1</name>
   <value>nn1:50070</value>
 </property>
```

#### 错误2
```
[root@nn1 ~]#  spark-shell --master yarn  --num-executors 3 --executor-memory 1g --deploy-mode client
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/06/28 16:56:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/06/28 16:56:16 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
17/06/28 16:56:32 ERROR cluster.YarnClientSchedulerBackend: Yarn application has already exited with state FAILED!
17/06/28 16:56:32 ERROR client.TransportClient: Failed to send RPC 7978295047055032075 to /10.255.20.190:49094: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)
17/06/28 16:56:32 ERROR cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(0,0,Map()) to AM was unsuccessful
java.io.IOException: Failed to send RPC 7978295047055032075 to /10.255.20.190:49094: java.nio.channels.ClosedChannelException
```
**解决方案**</br>
我在yarn-site.xml里面，加入了如下的配置：
```
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```
然后重启了yarn Cluster通过stop-yarn.sh，start-yarn.sh。
不过重启的时候，我发现yarn的Cluster本来就有问题，所以不晓得是重启生效还是修改的配置。
