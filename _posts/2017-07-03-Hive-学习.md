---
layout: post
title: "Hive 学习"
description: "Hive 学习"
category: articles
tags: [Spark]
comments: true
---
原文链接:
<http://dblab.xmu.edu.cn/blog/1086-2/>
<http://blog.csdn.net/yirenboy/article/details/47447489>

Hive 学习
==================
Hive安装在nn1,mysql安装在dn1
#### Sogou文件格式说明
文件中字段分别为：访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL
### 内部表
Hive的内部表与数据库中的Table在概念上是类似。每一个Table在Hive中都有一个相应的目录存储数据。例如一个表tbInner，它在HDFS中的路径为/user/hive/warehouse/tbInner </br>
内部表删除时，元数据与数据都会被删除
#### 从spark读取hive的table
```
scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

scala> val hive= SparkSession.builder().enableHiveSupport.getOrCreate
17/07/05 09:38:28 WARN sql.SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
hive: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@25c8c71e

scala> sql("show databases").show
+------------+
|databaseName|
+------------+
|     default|
|   sparktest|
+------------+


scala> val studentRDD = hive.sql("select * from sparktest.student").rdd
studentRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[8] at rdd at <console>:26

scala> studentRDD.foreach(t => println("Name:"+t(1)+",Gender:"+t(2)+",Age:"+t(3)))
Name:Weiliang,Gender:M,Age:24
Name:Xueqian,Gender:F,Age:23
```
#### 从spark写入数据到Hive的table中
```
scala> import org.apache.spark.sql.{SQLContext, Row}
import org.apache.spark.sql.{SQLContext, Row}

scala> import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}
import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}

scala> val studentRDD1 = sc.parallelize(Array("3 Rongcheng M 26","4 Guanhua M 27")).map(_.split(" "))
studentRDD1: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[10] at map at <console>:27

scala> val schema = StructType(List(StructField("id", IntegerType, true),StructField("name", StringType, true),StructField("gender", StringType, true),StructField("age", IntegerType, true)))

scala> val rowRDD = studentRDD1.map(p => Row(p(0).toInt, p(1).trim, p(2).trim, p(3).toInt))
rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[11] at map at <console>:29

scala> val studentDataFrame = spark.createDataFrame(rowRDD,schema)
studentDataFrame: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]
scala> studentDataFrame.createOrReplaceTempView("student")
scala> sql("select * from sparktest.student").show
+---+---------+------+---+
| id|     name|gender|age|
+---+---------+------+---+
|  1|  Xueqian|     F| 23|
|  2| Weiliang|     M| 24|
|  3|Rongcheng|     M| 26|
|  4|  Guanhua|     M| 27|
+---+---------+------+---+

hive> select * from sparktest.student;
OK
1       Xueqian F       23
2       Weiliang        M       24
3       Rongcheng       M       26
4       Guanhua M       27
Time taken: 1.785 seconds, Fetched: 4 row(s)
hive>
```
#### Hive使用sogou数据
```
hive> use sparktest
    > ;
OK
Time taken: 0.074 seconds
hive> CREATE TABLE SOGOUQ2(DT STRING,WEBSESSION STRING,WORD STRING,S_SEQ INT,C_SEQ INT,WEBSITE STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' ;
OK
Time taken: 0.498 seconds
hive> LOAD DATA LOCAL INPATH '/tian/data/soguo/S2.txt' INTO TABLE SOGOUQ2;
Loading data to table sparktest.sogouq2
OK
Time taken: 1.871 seconds
hive> select count(*) from SOGOUQ2;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20170705141555_3e9ddc25-03f9-46de-93ac-399f213d155f
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1498699565705_0008, Tracking URL = http://nn1:8088/proxy/application_1498699565705_0008/
Kill Command = /app/hadoop/bin/hadoop job  -kill job_1498699565705_0008
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2017-07-05 14:16:03,350 Stage-1 map = 0%,  reduce = 0%
2017-07-05 14:16:10,746 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.5 sec
2017-07-05 14:16:15,033 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.95 sec
MapReduce Total cumulative CPU time: 4 seconds 950 msec
Ended Job = job_1498699565705_0008
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.95 sec   HDFS Read: 229629059 HDFS Write: 107 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 950 msec
OK
2000000
Time taken: 21.966 seconds, Fetched: 1 row(s)
```
**比如查询包含baidu的数据**
```
hive> select count(*) from SOGOUQ2 where WEBSITE like '%baidu%'
    > ;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20170705141830_85718170-b60d-44e9-aa0f-e4c5a9ebcef1
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1498699565705_0009, Tracking URL = http://nn1:8088/proxy/application_1498699565705_0009/
Kill Command = /app/hadoop/bin/hadoop job  -kill job_1498699565705_0009
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2017-07-05 14:18:36,544 Stage-1 map = 0%,  reduce = 0%
2017-07-05 14:18:43,979 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.64 sec
2017-07-05 14:18:49,234 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.05 sec
MapReduce Total cumulative CPU time: 6 seconds 50 msec
Ended Job = job_1498699565705_0009
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.05 sec   HDFS Read: 229629854 HDFS Write: 106 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 50 msec
OK
260085
Time taken: 20.601 seconds, Fetched: 1 row(s)
```
**结果排名第1，点击次序排第2，其中URL包含baidu的数据**
```
hive> select count(*) from SOGOUQ2 where S_SEQ=1 and C_SEQ=2 and WEBSITE like '%baidu%';
```
### 外部表
外部表指向已经在HDFS中存在的数据，并可以创建Partition。它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。内部表的创建过程和数据加载过程这两个过程可以分别独立完成，也可以在同一个语句中完成，在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。而外部表只有一个过程，加载数据和创建表同时完成（CREATE EXTERNAL TABLE ……LOCATION），实际数据是存储在LOCATION后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。当删除一个External Table时，仅删除该链接。
#### 第一步 在HDFS创建外部表存放数据目录
```
[root@nn1 ~]#  hadoop fs -mkdir -p /class5/sogouq1
[root@nn1 ~]#  hadoop fs -ls /class5
Found 1 items
drwxr-xr-x   - root supergroup          0 2017-07-05 15:10 /class5/sogouq1
```
#### 第二步 在Hive创建外部表，指定表存放目录
[ROW FORMAT DELIMITED]关键字，是用来设置创建的表在加载数据的时候，支持的列分隔符。
```
hive> CREATE EXTERNAL TABLE SOGOUQ1(DT STRING,WEBSESSION STRING,WORD STRING,S_SEQ INT,C_SEQ INT,WEBSITE STRING) ROW FORMAT DELIMITED                       FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION '/class5/sogouq1';
OK
Time taken: 1.203 seconds
```
#### 第三步 加载数据文件到外部表对应的目录中，查询。
```
[root@nn1 ~]#  hadoop fs -copyFromLocal /tian/data/soguo/S1.txt  /class5/sogouq1/
```
查询前三行
```
hive> select * from SOGOUQ1 limit 3;
OK
20111230000005  57375476989eea12893c0c3811607bcf        奇艺高清        1       1       http://www.qiyi.com/
20111230000005  66c5bb7774e31d0a22278249b26bc83a        凡人修仙传      3       1       http://www.booksky.org/BookDetail.aspx?BookID=1050804&Level=1
20111230000007  b97920521c78de70ac38e3713f524b50        本本联盟        1       1       http://www.bblianmeng.com/
Time taken: 0.222 seconds, Fetched: 3 row(s)
```
可以看到非常快，Hive并没有生成job。可以看出Hive会根据查询不同任务决定是否生成Job，获取前10条并没有生成Job，而是得到数据后直接进行显示。
