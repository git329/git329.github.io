---
layout: post
title: "Spark+Hadoop Instllation"
description: "Spark"
category: articles
tags: [Spark]
comments: true
---
Spark+Hadoop Install with HA
============================
<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

			- [集群网络环境](#集群网络环境)
			- [设置Host映射文件](#设置host映射文件)
			- [配置SSH公钥登陆](#配置ssh公钥登陆)
			- [安装JDK](#安装jdk)


<!-- /TOC -->

####  参考
整个安装都是参考一下美文：
<https://www.server-world.info/en/note?os=CentOS_7&p=hadoop>
<https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/>
<http://blog.csdn.net/yirenboy/article/details/47296395>


####  集群网络环境

| Hostname | 类型 | 核数/内存 | 用户名 | 目录 | OS |
| :------- | :--- | :------- | :----- | :-- | :- |
| nn1      | Active NameNode,DataNode</br> Resource Manager</br>Master,Worker</br>Zookeeper | 2核/15G | root | /app/hadoop</br>/opt/scala | CentOS Linux 7 |
| nn2      | Standby NameNode,DataNode</br> Node Manager</br>Worker</br>Zookeeper | 2核/15G | root | /app/hadoop</br>/opt/scala | CentOS Linux 7 |
| dn1      | DataNode</br> Node Manager</br>Worker</br>Zookeeper | 2核/15G | root | /app/hadoop</br>/opt/scala | CentOS Linux 7 |

#### 设置Host映射文件
使用root身份编辑/etc/hosts映射文件，设置IP地址与机器名的映射，设置信息如下：
```
<ip address> nn1
<ip address> nn2
<ip address> dn1
```
#### 配置SSH公钥登陆
1. 以root用户分别登陆所有的node，修改/etc/ssh/sshd_config如下：
```
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile .ssh/authorized_keys
PasswordAuthentication yes
```
重启SSH服务如下：
```
# service sshd restart
```
2. 登陆nn1和nn2,创建key-pairs，然后传给其他node。
```
# ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
ac:50:0d:b7:69:d5:2f:a1:6a:56:08:a4:a6:b9:c9:68 root@euca-10-255-9-2.eucalyptus.internal
The key's randomart image is:
+--[ RSA 2048]----+
|     .o . ..     |
|     ..+ +  o    |
|    o ..=. . o   |
|   + . o. o . .  |
|  o .   So   .   |
| o o . .+        |
|.E+   .o         |
|.                |
|                 |
+-----------------+
# ssh-copy-id nn2
The authenticity of host 'nn2 (10.255.19.247)' can't be established.
ECDSA key fingerprint is f9:ec:05:56:6c:d5:3b:93:b8:7e:8f:5d:32:d3:58:1e.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
root@nn2's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'nn2'"
and check to make sure that only the key(s) you wanted were added.

```
在nn2上重复上面操作。

#### 安装JDK
1. 下载JDK
从<http://www.oracle.com/technetwork/java/javase/downloads/index.html>找到最新的JDK,下载
登陆nn1,下载JDK，如下：
```
# curl -LO -H "Cookie: oraclelicense=accept-securebackup-cookie"  "http://download.oracle.com/otn-pub/java/jdk/8u121-b13/e9e7ea248e2c4826b92b3f075a80e441/jdk-8u121-linux-x64.rpm"
```
SCP到其他node,
```
#scp jdk-8u121-linux-x64.rpm nn2:/root/
#scp jdk-8u121-linux-x64.rpm dn1:/root/
```
2. 安装JDK
```
#rpm -ivh
```
3. 配置/etc/profile
```
for host in nn1 nn2 dn1
do
ssh $host  "cat <<EOF >>/etc/profile
export JAVA_HOME=/usr/java/default
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
EOF
"
done
```
4. 如果系统上还安装了其他JAVA版本，用下面方法设置：
```
# alternatives --config java
```

#### 安装Scala
1. 下载Scala
```
# wget https://downloads.lightbend.com/scala/2.12.2/scala-2.12.2.tgz
#
```
2. 解压
```
#for host in nn1 nn2 dn1;do ssh $host "tar -zxvf /root/scala-2.12.2.tgz -C /opt/ --no-same-owner";done
#for host in nn1 nn2 dn1;do ssh $host "ln -s /opt/scala-2.12.2 /opt/scala";done
```
3. 配置环境变量

登陆各个node，在/etc/profile追加如下：
```
#SCALA setting
export SCALA_HOME=/opt/scala
export PATH=$SCALA_HOME/bin:$PATH
EOF
done
```

#### 安装配置Hadoop

#### #安装Hadoop
1. 登陆下载Hadoop：
```
 #curl -O http://apache.mirror.digionline.de/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
 #scp hadoop-2.7.3.tar.gz nn2:/root/
 #scp hadoop-2.7.3.tar.gz dn1:/root/
```
2. 在nn1上面都解压：
```
tar -xzvf hadoop-2.7.3.tar.gz  -C /app/hadoop/ --no-same-owner --strip-components 1
```
3. 配置环境变量
在/etc/profile追加如下：
```
#Hadoop setting
export HADOOP_HOME=/app/hadoop
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
```
#### #配置Hadoop
1. 修改hadoop-env.sh
```
#  sed -i -e 's/\${JAVA_HOME}/\/usr\/java\/default/' /app/hadoop/etc/hadoop/hadoop-env.sh
```
2. 在nn1上面配置/app/hadoop/etc/hadoop/core-site.xml
```
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://first-cluster</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/app/hadoop/first-cluster</value>
  </property>
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>nn1:2181,nn2:2181,dn1:2181</value>
  </property>
</configuration>
```

3. 在nn1上面配置/app/hadoop/etc/hadoop/hdfs-site.xml
```
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.nameservices</name>
    <!-- If there are several clusters, please use ',' to saparate them-->
    <value>first-cluster</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.first-cluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.first-cluster.name-node-1</name>
    <value>nn1:9000</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.first-cluster.name-node-1</name>
    <value>nn1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.first-cluster.name-node-2</name>
    <value>nn2:9000</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.first-cluster.name-node-2</name>
    <value>nn2:50070</value>
  </property>
  <property>
    <name>dfs.hosts</name>
    <value>/app/hadoop/etc/hadoop/slaves</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://nn1:8485;nn2:8485;dn1:8485/first-cluster</value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>${hadoop.tmp.dir}/journal-data</value>
  </property>
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.first-cluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
  </property>
</configuration>
```
4. 在nn1上面配置slave文件/app/hadoop/etc/hadoop/slaves，添加所有的datanode，文件内容如下：
```
nn1
nn2
dn1
```
5. 在nn1上面配置mapred-site.xml 和yarn-site.xml
/app/hadoop/etc/hadoop/yarn-site.xml
```
<configuration>
  <!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>nn1</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```
/app/hadoop/etc/hadoop/mapred-site.xml
```
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```
6. 拷贝/app/hadoop 到其他node
```
# scp -r /app/hadoop nn2:/app
# scp -r /app/hadoop dn1:/app
```
#####安装Zookeeper

1. 在nn1上面下载zookeeper包到目录/app/下面
```
# wget https://archive.apache.org/dist/zookeeper/stable/zookeeper-3.4.10.tar.gz
```
2. 解压到当前目录
```
[root@nn1 app]# tar -xvzf zookeeper-3.4.10.tar.gz
```
3. 配置/etc/profile
```
#Zookeeper setting
export export ZOOKEEPER_HOME=/app/zookeeper-3.4.10
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```
4. 创建目录用来存放zookeeper data
```
[root@nn1 zookeeper]# mkdir /app/zookeeper-3.4.10/data
```
5. 配置zoo.cfg
```
[root@nn1 zookeeper]# cp  /app/zookeeper-3.4.10/conf/zoo_sample.cfg /app/zookeeper-3.4.10/conf/zoo.cfg
```
修改zoo.cfg以下两项
```
dataDir=/app/zookeeper-3.4.10/data
server.1=nn1:2888:3888
server.2=nn2:2888:3888
server.3=dn1:2888:3888
```
6. 拷贝/app/zookeeper-3.4.10 和/etc/profile 到其他node
```
[root@nn1 /]#  scp -r /app/zookeeper-3.4.10 nn2:/app
[root@nn1 /]#  scp -r /app/zookeeper-3.4.10 dn1:/app
[root@nn1 /]# scp /etc/profile nn2:/etc
[root@nn1 /]# scp /etc/profile dn1:/etc
```
7. 创建myid
```
[root@nn1 /]# echo 1 >/app/zookeeper-3.4.10/data/myid
[root@nn2 /]# echo 2 >/app/zookeeper-3.4.10/data/myid
[root@dn1 /]# echo 3 >/app/zookeeper-3.4.10/data/myid
```
##### 启动hadoop和zookeeper
```
[root@nn1 /]# zkServer.sh start
[root@nn2 ~]# zkServer.sh start
[root@dn1 ~]# zkServer.sh start
[root@nn1 /]# hdfs zkfc -formatZK
[root@nn1 /]#  hadoop-daemon.sh start journalnode
[root@nn2 /]#  hadoop-daemon.sh start journalnode
[root@dn1 /]#  hadoop-daemon.sh start journalnode
[root@nn1 /]# hdfs namenode -format
[root@nn1 /]# hadoop-daemon.sh start namenode
[root@nn2 ~]# hdfs namenode -bootstrapStandby
[root@nn1 /]# hadoop-daemon.sh stop  namenode
[root@nn1 ~]# start-dfs.sh
Starting namenodes on [nn1 nn2]
nn1: starting namenode, logging to /app/hadoop/logs/hadoop-root-namenode-nn1.out
nn2: starting namenode, logging to /app/hadoop/logs/hadoop-root-namenode-nn2.out
nn2: starting datanode, logging to /app/hadoop/logs/hadoop-root-datanode-nn2.out
nn1: starting datanode, logging to /app/hadoop/logs/hadoop-root-datanode-nn1.out
dn1: starting datanode, logging to /app/hadoop/logs/hadoop-root-datanode-dn1.out
Starting journal nodes [nn1 nn2 dn1]
dn1: starting journalnode, logging to /app/hadoop/logs/hadoop-root-journalnode-dn1.out
nn1: starting journalnode, logging to /app/hadoop/logs/hadoop-root-journalnode-nn1.out
nn2: starting journalnode, logging to /app/hadoop/logs/hadoop-root-journalnode-nn2.out
Starting ZK Failover Controllers on NN hosts [nn1 nn2]
nn1: starting zkfc, logging to /app/hadoop/logs/hadoop-root-zkfc-nn1.out
nn2: starting zkfc, logging to /app/hadoop/logs/hadoop-root-zkfc-nn2.out
[root@nn1 ~]# start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /app/hadoop/logs/yarn-root-resourcemanager-nn1.out
nn2: starting nodemanager, logging to /app/hadoop/logs/yarn-root-nodemanager-nn2.out
dn1: starting nodemanager, logging to /app/hadoop/logs/yarn-root-nodemanager-dn1.out
nn1: starting nodemanager, logging to /app/hadoop/logs/yarn-root-nodemanager-nn1.out
[root@nn1 ~]# jps
23508 Jps
23109 ResourceManager
20598 QuorumPeerMain
22519 NameNode
23000 DFSZKFailoverController
22619 DataNode
23211 NodeManager
22815 JournalNode
```
##### 验证hadoop是否安装成功
```
[root@nn1 ~]# hdfs haadmin -getServiceState nn1
active
[root@nn1 ~]# hdfs haadmin -getServiceState nn2
standby
[root@nn1 ~]# hdfs dfs -mkdir /test
[root@nn1 ~]# hdfs dfs -copyFromLocal /app/hadoop/NOTICE.txt /test
[root@nn1 ~]# hdfs dfs -cat /test/NOTICE.txt
[root@nn1 ~]# hadoop jar /app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar  wordcount /test/NOTICE.txt /output01
[root@nn1 ~]# hadoop fs -ls /output01
Found 2 items
-rw-r--r--   3 root supergroup          0 2017-06-01 17:24 /output01/_SUCCESS
-rw-r--r--   3 root supergroup       8969 2017-06-01 17:24 /output01/part-r-00000
```
##### 登陆Hadoop Web
http://nn1:50070/
http://nn1:8088/

#### 安装Spark
#### #下载Spark
登陆https://spark.apache.org/downloads.html 获取Spark的下载地址。在nn1上面获取Spark包，如下
```
[root@nn1 ~]# wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz
```
##### 安装Spark
```
[root@nn1 ~]# mkdir /app/spark/
[root@nn1 ~]# tar -xzvf spark-2.1.1-bin-hadoop2.7.tgz  -C /app/spark/ --strip-components 1
```
##### 配置
1. 配置/etc/profile
```
export SPARK_HOME=/app/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```
2. 配置/app/spark/conf/slaves
```
# A Spark Worker will be started on each of the machines listed below.
nn1
nn2
dn1
```
3. 配置/app/spark/conf/spark-env.sh
```
[root@nn1 ~]# cp /app/spark/conf/spark-env.sh.template /app/spark/conf/spark-env.sh
```
加入Spark环境配置内容，设置nn1为Master节点:
```
export SPARK_MASTER_IP=nn1
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=1
export SPARK_WORKER_MEMORY=1g
export SPARK_EXECUTOR_CORES=1
export SPARK_EXECUTOR_INSTANCES=1
export SPARK_EXECUTOR_MEMORY=1g
export SPARK_DRIVER_MEMORY=1g
```

##### 向各节点分发Spark程序
```
[root@nn1 app]# scp -r spark root@nn2:/app/
[root@nn1 app]# scp -r spark root@dn1:/app/
[root@nn1 app]# scp /etc/profile nn2:/etc/
[root@nn1 app]# scp /etc/profile dn1:/etc/
```
##### 启动Spark
```
[root@nn1 app]# /app/spark/sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /app/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-nn1.out
nn1: starting org.apache.spark.deploy.worker.Worker, logging to /app/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-nn1.out
dn1: starting org.apache.spark.deploy.worker.Worker, logging to /app/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-dn1.out
nn2: starting org.apache.spark.deploy.worker.Worker, logging to /app/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-nn2.out
```
##### 验证
###### 测试web
在浏览器中输入 http://nn1:8080
###### 测试spark-shell
使用spark-shell连接集群
```
[root@nn1 ~]# /app/spark/bin/spark-shell
scala> val rdd=sc.textFile("hdfs://nn1:9000/test/NOTICE.txt")
rdd: org.apache.spark.rdd.RDD[String] = hdfs://nn1:9000/test/NOTICE.txt MapPartitionsRDD[1] at textFile at <console>:24
scala> rdd.cache()
res0: rdd.type = hdfs://nn1:9000/test/NOTICE.txt MapPartitionsRDD[1] at textFile at <console>:24
scala> val wordCount=rdd.flatMap(_.split(" ")).map(x=>(x,1)).reduceByKey(_+_)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:26
scala>  wordCount.take(10)
res0: Array[(String, Int)] = Array((FuseSource,2), (2.6,,1), (package,1), (2008,1), (under,6), (Unless,1), (license/LICENSE.jbzip2.txt,1), (Lea,,1), (J.,1), (improvement,1))
```
当然上面的实现也可以一句话来实现：
```
scala> sc.textFile("hdfs://nn1:9000/test/NOTICE.txt").flatMap(_.split(" ")).map(x=>(x,1)).reduceByKey(_+_).map(x=>(x._2,x._1)).sortByKey(false).map(x=>(x._2,x._1)).take(10)
res0: Array[(String, Int)] = Array(("",903), (*,125), (the,44), (of,44), (product,43), (This,35), (The,29), (a,29), (which,27), (be,25))
scala>
```
上面的执行结果就是：
Array(("",903), (\*,125), (the,44), (of,44), (product,43), (This,35), (The,29), (a,29), (which,27), (be,25))
####### 测试spark-submit
```
[root@nn1 ~]# /app/spark/bin/spark-submit --class org.apache.spark.examples.SparkPi /app/spark/examples/jars/spark-examples_2.11-2.1.1.jar 200
```
