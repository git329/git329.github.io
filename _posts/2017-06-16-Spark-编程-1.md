---
layout: post
title: Spark编程-1
description: "Spark编程"
category: articles
tags: [Spark编程]
comments: true
---
Spark编程-1
=======
#### Reference
<http://blog.csdn.net/yirenboy/article/details/47429033>
<http://www.jianshu.com/p/4c5c2e535da5>
#### 前言
这个是我的spark编程学习笔记。</br>
#### 数据准备
从引用的那篇美文下载的数据文件，解压以后，我遇到乱码问题，解决方式如下：
```
[root@nn1 soguo]#   iconv -f gb18030 -t utf-8 <SogouQ2.txt >S2.txt
```
然后把下载的两个数据文件上传到hdfs，做准备
```
hadoop fs -mkdir /sogou
hadoop fs -put sogou/S1.txt /sogou
hadoop fs -put sogou/S2.txt /sogou
[root@nn1 ~]#  hadoop fs -ls /sogou
Found 2 items
-rw-r--r--   3 root supergroup  114845850 2017-06-16 09:20 /sogou/S1.txt
-rw-r--r--   3 root supergroup  229616757 2017-06-16 09:20 /sogou/S2.txt

```
#### 文件例子读取
第一步：读取文件
```
scala> val text=sc.textFile("hdfs://nn1:9000/sogou/")
text: org.apache.spark.rdd.RDD[String] = hdfs://nn1:9000/sogou/ MapPartitionsRDD[6] at textFile at <console>:24

scala> text.toDebugString
res2: String =
(3) hdfs://nn1:9000/sogou/ MapPartitionsRDD[6] at textFile at <console>:24 []
 |  hdfs://nn1:9000/sogou/ HadoopRDD[5] at textFile at <console>:24 []

```
RDD类型转换过程是HadoopRDD->MapPartitionsRDD,为什么呢？
这是因为Spark的源码就是这样写的啊，返回的就是如下你可以看到使用了map方法转换成了MapPartitionsRDD：
```
/**
   * Read a text file from HDFS, a local file system (available on all nodes), or any
   * Hadoop-supported file system URI, and return it as an RDD of Strings.
   * @param path path to the text file on a supported file system
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of lines of the text file
   */
  def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] =  withScope {
    assertNotStopped()
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
      minPartitions).map(pair => pair._2.toString).setName(path)
  }
```
第二步：将文本按照“\t”拆分，并且过滤出只有6个字段的
```
scala> val rdd2=text.map(_.split("\t")).filter(_.length==6)
rdd2: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[22] at filter at <console>:26
```
第三步：
```
scala> rdd2.map(x=>(x(1),1)).reduceByKey(_+_).map(x=>(x._2,x._1)).sortByKey(false).map(x=>(x._2,x._1)).saveAsTextFile("hdfs://nn1:9000/sogou/output1")
```
第四步：合并结果，放到本地
```
hadoop fs -getmerge /sogou/output1 /local/result
[root@nn1 ~]#  head -5 /local/result
(b3c94c37fb154d46c30a360c7941ff7e,676)
(cc7063efc64510c20bcdd604e12a3b26,613)
(955c6390c02797b3558ba223b8201915,391)
(b1e371de5729cdda9270b7ad09484c4f,337)
(6056710d9eafa569ddc800fe24643051,277)

```
#### 不同的deploy模式
1.  Standalone模式
[root@nn1 ~]#  spark-shell --master spark://nn1:7077 --executor-memory 1g
2. YARN-Client模式
[root@nn1 ~]#  spark-shell --master yarn  --num-executors 3 --executor-memory 1g --deploy-mode client
